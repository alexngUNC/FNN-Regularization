{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 50000\n",
      "Testing set: 10000\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Tensorflow aliases\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert to float32 for scaling\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize data by scaling by max\n",
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()\n",
    "\n",
    "# One-hot encode target labels and reshape\n",
    "y_train = np_utils.to_categorical(y_train.transpose()).reshape(50000,10)\n",
    "y_test = np_utils.to_categorical(y_test.transpose()).reshape(10000,10)\n",
    "input_shape = (32,32,3)\n",
    "\n",
    "print(f\"Training set: {len(X_train)}\\nTesting set: {len(X_test)}\")\n",
    "\n",
    "def build_cnn2(activation='relu', kernel_size=(3,3), pool_size=(2,2), regularizer=None, lam=0.01, summarize=False, dropout=False, drop_prob1=0.2, drop_prob2=0.2):\n",
    "  model = Sequential()\n",
    "  \n",
    "  if regularizer == None:\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(filters=32, kernel_size=kernel_size, activation=activation, input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPool2D(pool_size))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=kernel_size, activation=activation))\n",
    "    model.add(MaxPool2D(pool_size))\n",
    "    if dropout:\n",
    "      model.add(Dropout(drop_prob1))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=kernel_size, activation=activation))\n",
    "    model.add(MaxPool2D(pool_size))\n",
    "    if dropout:\n",
    "      model.add(Dropout(drop_prob2))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=activation))\n",
    "    if dropout:\n",
    "      model.add(Dropout(drop_prob2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    if summarize:\n",
    "      model.build()\n",
    "      model.summary()\n",
    "    return model\n",
    "  else:\n",
    "    # Convolutional layers\n",
    "    model.add(Conv2D(filters=32, kernel_size=kernel_size, kernel_regularizer=regularizer(lam), activation=activation, input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPool2D(pool_size))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=kernel_size, kernel_regularizer=regularizer(lam), activation=activation))\n",
    "    model.add(MaxPool2D(pool_size))\n",
    "    if dropout:\n",
    "      model.add(Dropout(drop_prob1))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=kernel_size, kernel_regularizer=regularizer(lam), activation=activation))\n",
    "    model.add(MaxPool2D(pool_size))\n",
    "    if dropout:\n",
    "      model.add(Dropout(drop_prob2))\n",
    "\n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=activation, kernel_regularizer=regularizer(lam)))\n",
    "    if dropout:\n",
    "      model.add(Dropout(drop_prob2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if summarize:\n",
    "      model.build()\n",
    "      model.summary()\n",
    "    return model\n",
    "\n",
    "# Helper functions\n",
    "def show_acc(h, e):\n",
    "  plt.plot(range(e), h.history['accuracy'], label='Training')\n",
    "  plt.plot(range(e), h.history['val_accuracy'], label='Validation')\n",
    "  plt.ylim([0, 1])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "def build_and_plot(activation='relu', kernel_size=(3,3), pool_size=(2,2), regularizer=None, lam=0.01, dropout=False, drop_prob1=0.2, drop_prob2=0.2, summarize=False, e=100, verbose=0):\n",
    "  model = build_cnn2(activation=activation, kernel_size=kernel_size, pool_size=pool_size, regularizer=regularizer, lam=lam, dropout=dropout, drop_prob1=drop_prob1, drop_prob2=drop_prob2, summarize=summarize)\n",
    "  history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=e, verbose=verbose)\n",
    "  show_acc(history, e)\n",
    "  print(\"Max accuracy: \", np.max(history.history['accuracy']))\n",
    "  print(\"Min accuracy: \", np.min(history.history['accuracy']))\n",
    "  print(\"Test performance \", model.evaluate(X_test, y_test))\n",
    "  return model, history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same dropout probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop_1, drop_1_history = build_and_plot(dropout=True, drop_prob1=0.1, drop_prob2=0.1, e=100)\n",
    "# drop_2, drop_2_history = build_and_plot(dropout=True, drop_prob1=0.2, drop_prob2=0.2, e=100)\n",
    "# drop_3, drop_3_history = build_and_plot(dropout=True, drop_prob1=0.3, drop_prob2=0.3, e=100)\n",
    "# drop_4, drop_4_history = build_and_plot(dropout=True, drop_pro1b=0.4, drop_prob2=0.4, e=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller then larger dropout probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop_s1, drop_s1_history = build_and_plot(dropout=True, drop_prob1=0.1, drop_prob2=0.2, e=100)\n",
    "# drop_s2, drop_s2_history = build_and_plot(dropout=True, drop_prob1=0.2, drop_prob2=0.3, e=100)\n",
    "# drop_s3, drop_s3_history = build_and_plot(dropout=True, drop_prob1=0.4, drop_prob2=0.5, e=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger then smaller dropout probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #drop_b1, drop_b1_history = build_and_plot(dropout=True, drop_prob1=0.2, drop_prob2=0.1, e=100)\n",
    "# drop_b2, drop_b2_history = build_and_plot(dropout=True, drop_prob1=0.3, drop_prob2=0.2, e=100)\n",
    "# drop_b3, drop_b3_history = build_and_plot(dropout=True, drop_prob1=0.4, drop_prob2=0.3, e=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
